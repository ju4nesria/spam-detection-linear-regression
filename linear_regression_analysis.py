"""
üéØ AN√ÅLISIS COMPLETO DE REGRESI√ìN LINEAL PARA DETECCI√ìN DE SPAM
================================================================

Este script realiza un an√°lisis completo de regresi√≥n lineal para detecci√≥n de spam incluyendo:
- Construcci√≥n de modelo con 1000 instancias
- Matriz de confusi√≥n
- An√°lisis de manipulaci√≥n de datos
- Reporte de importancia de features (top 10)
- An√°lisis de correlaci√≥n con Python, Keras y sklearn
- Generaci√≥n de reportes en PDF

Autor: Juan Esteban Moya Ria√±o, Maryuri Espinosa
Fecha: 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import (mean_squared_error, r2_score, accuracy_score, 
                           confusion_matrix, classification_report, 
                           mean_absolute_error)
from sklearn.preprocessing import StandardScaler
import pickle
import re
import warnings
from datetime import datetime
import os

# Para an√°lisis de correlaci√≥n con Keras
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.optimizers import Adam
    KERAS_AVAILABLE = True
except ImportError:
    KERAS_AVAILABLE = False
    print("‚ö†Ô∏è Keras/TensorFlow no disponible. An√°lisis de correlaci√≥n limitado.")

warnings.filterwarnings('ignore')

class LinearRegressionSpamAnalysis:
    """
    An√°lisis completo de regresi√≥n lineal para detecci√≥n de spam.
    
    Incluye:
    - Preprocesamiento de datos
    - Construcci√≥n de modelo con 1000 instancias
    - Matriz de confusi√≥n
    - An√°lisis de importancia de features
    - An√°lisis de correlaci√≥n
    - Generaci√≥n de reportes
    """
    
    def __init__(self):
        self.df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.vectorizer = None
        self.model = None
        self.feature_names = []
        self.scaler = StandardScaler()
        self.results = {}
        
    def load_and_preprocess_data(self, file_path, max_instances=1000):
        """
        Carga y preprocesa los datos para el an√°lisis.
        
        Args:
            file_path (str): Ruta al archivo CSV
            max_instances (int): N√∫mero m√°ximo de instancias a usar
        """
        print("üìÇ Cargando y preprocesando datos...")
        
        # Cargar datos
        self.df = pd.read_csv(file_path)
        
        # Limitar a 1000 instancias si hay m√°s
        if len(self.df) > max_instances:
            print(f"üìä Limitando dataset a {max_instances} instancias")
            self.df = self.df.sample(n=max_instances, random_state=42)
        
        print(f"‚úÖ Dataset cargado: {len(self.df)} instancias")
        print(f"   - HAM: {sum(self.df['label'] == 'ham')}")
        print(f"   - SPAM: {sum(self.df['label'] == 'spam')}")
        
        # Preprocesar texto
        self.df['clean_text'] = self.df['email_text'].apply(self._preprocess_text)
        
        # Crear features
        self._create_features()
        
        # Preparar target
        self.y = (self.df['label'] == 'spam').astype(int)
        
        # Dividir datos
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y
        )
        
        print(f"‚úÖ Datos preparados:")
        print(f"   - Entrenamiento: {self.X_train.shape[0]} instancias")
        print(f"   - Prueba: {self.X_test.shape[0]} instancias")
        
    def _preprocess_text(self, text):
        """Preprocesa el texto para an√°lisis."""
        # Convertir a min√∫sculas
        text = text.lower()
        
        # Eliminar puntuaci√≥n
        text = re.sub(r'[^\w\s]', '', text)
        
        # Eliminar n√∫meros
        text = re.sub(r'\d+', '', text)
        
        # Limpiar espacios
        text = ' '.join(text.split())
        
        return text
    
    def _create_features(self):
        """Crea features usando CountVectorizer."""
        print("üîß Creando features...")
        
        self.vectorizer = CountVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        
        self.X = self.vectorizer.fit_transform(self.df['clean_text'])
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"‚úÖ Features creadas: {self.X.shape[1]} caracter√≠sticas")
    
    def train_linear_regression_model(self):
        """Entrena el modelo de regresi√≥n lineal."""
        print("\nüéØ ENTRENANDO MODELO DE REGRESI√ìN LINEAL")
        print("=" * 50)
        
        # Crear y entrenar modelo
        self.model = LinearRegression()
        self.model.fit(self.X_train, self.y_train)
        
        # Hacer predicciones
        y_pred_train = self.model.predict(self.X_train)
        y_pred_test = self.model.predict(self.X_test)
        
        # Convertir predicciones continuas a clases
        y_pred_classes = (y_pred_test > 0.5).astype(int)
        
        # Calcular m√©tricas
        train_mse = mean_squared_error(self.y_train, y_pred_train)
        test_mse = mean_squared_error(self.y_test, y_pred_test)
        train_r2 = r2_score(self.y_train, y_pred_train)
        test_r2 = r2_score(self.y_test, y_pred_test)
        accuracy = accuracy_score(self.y_test, y_pred_classes)
        mae = mean_absolute_error(self.y_test, y_pred_test)
        
        # Guardar resultados
        self.results = {
            'train_mse': train_mse,
            'test_mse': test_mse,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'accuracy': accuracy,
            'mae': mae,
            'y_pred_test': y_pred_test,
            'y_pred_classes': y_pred_classes
        }
        
        print(f"üìà M√âTRICAS DEL MODELO:")
        print(f"   - MSE (Train): {train_mse:.4f}")
        print(f"   - MSE (Test): {test_mse:.4f}")
        print(f"   - R¬≤ (Train): {train_r2:.4f}")
        print(f"   - R¬≤ (Test): {test_r2:.4f}")
        print(f"   - Accuracy: {accuracy:.4f}")
        print(f"   - MAE: {mae:.4f}")
        
        return self.results
    
    def generate_confusion_matrix(self):
        """Genera y muestra la matriz de confusi√≥n."""
        print("\nüìä GENERANDO MATRIZ DE CONFUSI√ìN")
        print("=" * 40)
        
        # Calcular matriz de confusi√≥n
        cm = confusion_matrix(self.y_test, self.results['y_pred_classes'])
        
        # Crear visualizaci√≥n
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['HAM', 'SPAM'], 
                   yticklabels=['HAM', 'SPAM'])
        plt.title('Matriz de Confusi√≥n - Regresi√≥n Lineal')
        plt.xlabel('Predicci√≥n')
        plt.ylabel('Valor Real')
        plt.tight_layout()
        plt.savefig('confusion_matrix_linear.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Mostrar m√©tricas detalladas
        tn, fp, fn, tp = cm.ravel()
        
        print(f"üìà M√âTRICAS DETALLADAS:")
        print(f"   - Verdaderos Negativos (HAM correctos): {tn}")
        print(f"   - Falsos Positivos (HAM clasificados como SPAM): {fp}")
        print(f"   - Falsos Negativos (SPAM clasificados como HAM): {fn}")
        print(f"   - Verdaderos Positivos (SPAM correctos): {tp}")
        
        # Calcular m√©tricas adicionales
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"   - Precisi√≥n: {precision:.4f}")
        print(f"   - Recall: {recall:.4f}")
        print(f"   - F1-Score: {f1:.4f}")
        
        return cm
    
    def analyze_feature_importance(self, top_n=10):
        """Analiza la importancia de las features."""
        print(f"\nüîç AN√ÅLISIS DE IMPORTANCIA DE FEATURES (Top {top_n})")
        print("=" * 60)
        
        # Obtener coeficientes
        coefficients = self.model.coef_
        
        # Crear DataFrame con importancia
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'coefficient': coefficients,
            'abs_coefficient': np.abs(coefficients)
        })
        
        # Ordenar por importancia absoluta
        feature_importance = feature_importance.sort_values('abs_coefficient', ascending=False)
        
        # Top features
        top_features = feature_importance.head(top_n)
        
        print(f"üèÜ TOP {top_n} FEATURES M√ÅS IMPORTANTES:")
        print("-" * 50)
        
        for i, (_, row) in enumerate(top_features.iterrows(), 1):
            direction = "‚Üí SPAM" if row['coefficient'] > 0 else "‚Üí HAM"
            print(f"{i:2d}. {row['feature']:20} | {row['coefficient']:8.4f} | {direction}")
        
        # Crear visualizaci√≥n
        plt.figure(figsize=(12, 8))
        colors = ['red' if x > 0 else 'blue' for x in top_features['coefficient']]
        
        bars = plt.barh(range(len(top_features)), top_features['coefficient'], color=colors, alpha=0.7)
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Coeficiente (Importancia)')
        plt.title(f'Top {top_n} Features M√°s Importantes\n(Rojo=SPAM, Azul=HAM)')
        plt.grid(axis='x', alpha=0.3)
        
        # Agregar valores en las barras
        for i, (bar, coef) in enumerate(zip(bars, top_features['coefficient'])):
            plt.text(coef + (0.001 if coef > 0 else -0.001), i, f'{coef:.3f}', 
                    va='center', ha='left' if coef > 0 else 'right')
        
        plt.tight_layout()
        plt.savefig('feature_importance_linear.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return top_features
    
    def correlation_analysis(self):
        """Realiza an√°lisis de correlaci√≥n usando Python, Keras y sklearn."""
        print("\nüîó AN√ÅLISIS DE CORRELACI√ìN")
        print("=" * 40)
        
        # Convertir matriz dispersa a densa para an√°lisis de correlaci√≥n
        X_dense = self.X.toarray()
        
        # 1. Correlaci√≥n entre features (usando sklearn)
        print("üìä 1. Correlaci√≥n entre features (sklearn):")
        
        # Calcular matriz de correlaci√≥n
        corr_matrix = np.corrcoef(X_dense.T)
        
        # Encontrar correlaciones altas
        high_corr_pairs = []
        for i in range(len(corr_matrix)):
            for j in range(i+1, len(corr_matrix)):
                if abs(corr_matrix[i, j]) > 0.5:  # Umbral de correlaci√≥n
                    high_corr_pairs.append({
                        'feature1': self.feature_names[i],
                        'feature2': self.feature_names[j],
                        'correlation': corr_matrix[i, j]
                    })
        
        # Ordenar por correlaci√≥n absoluta
        high_corr_pairs.sort(key=lambda x: abs(x['correlation']), reverse=True)
        
        print(f"   Encontradas {len(high_corr_pairs)} correlaciones altas (>0.5):")
        for pair in high_corr_pairs[:10]:  # Mostrar top 10
            print(f"   - {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.3f}")
        
        # 2. An√°lisis con Keras (si est√° disponible)
        if KERAS_AVAILABLE:
            print("\nüß† 2. An√°lisis de correlaci√≥n con Keras:")
            self._keras_correlation_analysis(X_dense)
        else:
            print("\n‚ö†Ô∏è Keras no disponible. Saltando an√°lisis con redes neuronales.")
        
        # 3. Visualizaci√≥n de correlaci√≥n
        self._plot_correlation_matrix(corr_matrix)
        
        return high_corr_pairs
    
    def _keras_correlation_analysis(self, X_dense):
        """An√°lisis de correlaci√≥n usando Keras."""
        try:
            # Crear modelo simple para an√°lisis de correlaci√≥n
            model = Sequential([
                Dense(64, activation='relu', input_shape=(X_dense.shape[1],)),
                Dense(32, activation='relu'),
                Dense(1, activation='sigmoid')
            ])
            
            model.compile(optimizer=Adam(learning_rate=0.001), 
                         loss='binary_crossentropy', 
                         metrics=['accuracy'])
            
            # Entrenar modelo
            print("   Entrenando modelo Keras para an√°lisis de correlaci√≥n...")
            history = model.fit(X_dense, self.y, epochs=10, batch_size=32, 
                              validation_split=0.2, verbose=0)
            
            # Obtener pesos de la primera capa
            weights = model.layers[0].get_weights()[0]
            
            # Calcular importancia de features basada en pesos
            feature_importance_keras = np.mean(np.abs(weights), axis=1)
            
            # Top features seg√∫n Keras
            top_indices = np.argsort(feature_importance_keras)[-10:]
            print("   Top 10 features seg√∫n Keras:")
            for i, idx in enumerate(reversed(top_indices)):
                print(f"   {i+1:2d}. {self.feature_names[idx]:20} | {feature_importance_keras[idx]:.4f}")
            
            return feature_importance_keras
            
        except Exception as e:
            print(f"   ‚ùå Error en an√°lisis Keras: {e}")
            return None
    
    def _plot_correlation_matrix(self, corr_matrix):
        """Visualiza la matriz de correlaci√≥n."""
        # Seleccionar solo las primeras 50 features para visualizaci√≥n
        n_features = min(50, len(self.feature_names))
        corr_subset = corr_matrix[:n_features, :n_features]
        feature_names_subset = self.feature_names[:n_features]
        
        plt.figure(figsize=(15, 12))
        sns.heatmap(corr_subset, 
                   xticklabels=feature_names_subset,
                   yticklabels=feature_names_subset,
                   cmap='coolwarm', 
                   center=0,
                   square=True,
                   cbar_kws={'shrink': 0.8})
        plt.title('Matriz de Correlaci√≥n entre Features\n(Primeras 50 features)')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_data_manipulation_report(self):
        """Genera reporte de manipulaci√≥n de datos."""
        print("\nüìã REPORTE DE MANIPULACI√ìN DE DATOS")
        print("=" * 50)
        
        report = {
            'total_instances': len(self.df),
            'ham_instances': sum(self.df['label'] == 'ham'),
            'spam_instances': sum(self.df['label'] == 'spam'),
            'features_created': self.X.shape[1],
            'train_size': self.X_train.shape[0],
            'test_size': self.X_test.shape[0],
            'preprocessing_steps': [
                'Conversi√≥n a min√∫sculas',
                'Eliminaci√≥n de puntuaci√≥n',
                'Eliminaci√≥n de n√∫meros',
                'Limpieza de espacios',
                'Vectorizaci√≥n con CountVectorizer',
                'Eliminaci√≥n de stop words en ingl√©s',
                'N-gramas (1,2)',
                'L√≠mite de 1000 features'
            ]
        }
        
        print(f"üìä ESTAD√çSTICAS DEL DATASET:")
        print(f"   - Total de instancias: {report['total_instances']}")
        print(f"   - Instancias HAM: {report['ham_instances']} ({report['ham_instances']/report['total_instances']*100:.1f}%)")
        print(f"   - Instancias SPAM: {report['spam_instances']} ({report['spam_instances']/report['total_instances']*100:.1f}%)")
        print(f"   - Features creadas: {report['features_created']}")
        print(f"   - Datos de entrenamiento: {report['train_size']}")
        print(f"   - Datos de prueba: {report['test_size']}")
        
        print(f"\nüîß PASOS DE PREPROCESAMIENTO:")
        for i, step in enumerate(report['preprocessing_steps'], 1):
            print(f"   {i}. {step}")
        
        return report
    
    def generate_pdf_report(self, top_features):
        """Genera reporte en PDF con las features m√°s importantes."""
        print("\nüìÑ GENERANDO REPORTE PDF")
        print("=" * 30)
        
        try:
            from reportlab.lib.pagesizes import letter
            from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
            from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
            from reportlab.lib import colors
            from reportlab.lib.units import inch
            
            # Crear documento PDF
            doc = SimpleDocTemplate("feature_importance_report.pdf", pagesize=letter)
            styles = getSampleStyleSheet()
            story = []
            
            # T√≠tulo
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=16,
                spaceAfter=30,
                alignment=1  # Centrado
            )
            
            story.append(Paragraph("REPORTE DE IMPORTANCIA DE FEATURES", title_style))
            story.append(Paragraph("Sistema de Detecci√≥n de Spam - Regresi√≥n Lineal", styles['Heading2']))
            story.append(Spacer(1, 20))
            
            # Informaci√≥n del modelo
            story.append(Paragraph("INFORMACI√ìN DEL MODELO", styles['Heading2']))
            story.append(Paragraph(f"‚Ä¢ Algoritmo: Regresi√≥n Lineal", styles['Normal']))
            story.append(Paragraph(f"‚Ä¢ Instancias de entrenamiento: {self.X_train.shape[0]}", styles['Normal']))
            story.append(Paragraph(f"‚Ä¢ Instancias de prueba: {self.X_test.shape[0]}", styles['Normal']))
            story.append(Paragraph(f"‚Ä¢ Total de features: {len(self.feature_names)}", styles['Normal']))
            story.append(Paragraph(f"‚Ä¢ Precisi√≥n: {self.results['accuracy']:.4f}", styles['Normal']))
            story.append(Paragraph(f"‚Ä¢ R¬≤ Score: {self.results['test_r2']:.4f}", styles['Normal']))
            story.append(Spacer(1, 20))
            
            # Top features
            story.append(Paragraph("TOP 10 FEATURES M√ÅS IMPORTANTES", styles['Heading2']))
            
            # Crear tabla de features
            table_data = [['Rank', 'Feature', 'Coeficiente', 'Direcci√≥n', 'Importancia']]
            for i, (_, row) in enumerate(top_features.iterrows(), 1):
                direction = "SPAM" if row['coefficient'] > 0 else "HAM"
                importance = "Alta" if abs(row['coefficient']) > 0.01 else "Media" if abs(row['coefficient']) > 0.005 else "Baja"
                table_data.append([
                    str(i),
                    row['feature'],
                    f"{row['coefficient']:.4f}",
                    direction,
                    importance
                ])
            
            table = Table(table_data)
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(table)
            story.append(Spacer(1, 20))
            
            # Interpretaci√≥n
            story.append(Paragraph("INTERPRETACI√ìN", styles['Heading2']))
            story.append(Paragraph("‚Ä¢ Coeficiente positivo: La feature indica SPAM", styles['Normal']))
            story.append(Paragraph("‚Ä¢ Coeficiente negativo: La feature indica HAM", styles['Normal']))
            story.append(Paragraph("‚Ä¢ Valor absoluto mayor: Mayor importancia en la decisi√≥n", styles['Normal']))
            story.append(Paragraph("‚Ä¢ Las features se obtuvieron mediante CountVectorizer con n-gramas (1,2)", styles['Normal']))
            
            # Construir PDF
            doc.build(story)
            print("‚úÖ Reporte PDF generado: feature_importance_report.pdf")
            
        except ImportError:
            print("‚ùå reportlab no disponible. Instalando...")
            import subprocess
            subprocess.check_call(['pip', 'install', 'reportlab'])
            print("‚úÖ reportlab instalado. Ejecuta el script nuevamente.")
        except Exception as e:
            print(f"‚ùå Error generando PDF: {e}")
    
    def save_model_and_results(self):
        """Guarda el modelo y resultados."""
        print("\nüíæ GUARDANDO MODELO Y RESULTADOS")
        print("=" * 40)
        
        # Guardar modelo
        model_data = {
            'vectorizer': self.vectorizer,
            'model': self.model,
            'feature_names': self.feature_names,
            'results': self.results,
            'timestamp': datetime.now().isoformat()
        }
        
        with open('linear_regression_model.pkl', 'wb') as f:
            pickle.dump(model_data, f)
        
        print("‚úÖ Modelo guardado: linear_regression_model.pkl")
        
        # Guardar resultados en CSV
        results_df = pd.DataFrame([self.results])
        results_df.to_csv('model_results.csv', index=False)
        print("‚úÖ Resultados guardados: model_results.csv")

def main():
    """Funci√≥n principal para ejecutar el an√°lisis completo."""
    print("üöÄ AN√ÅLISIS COMPLETO DE REGRESI√ìN LINEAL PARA DETECCI√ìN DE SPAM")
    print("=" * 70)
    
    # Crear instancia del analizador
    analyzer = LinearRegressionSpamAnalysis()
    
    # 1. Cargar y preprocesar datos
    analyzer.load_and_preprocess_data('spam_ham_dataset_labeled.csv', max_instances=1000)
    
    # 2. Entrenar modelo
    results = analyzer.train_linear_regression_model()
    
    # 3. Generar matriz de confusi√≥n
    cm = analyzer.generate_confusion_matrix()
    
    # 4. Analizar importancia de features
    top_features = analyzer.analyze_feature_importance(top_n=10)
    
    # 5. An√°lisis de correlaci√≥n
    correlation_results = analyzer.correlation_analysis()
    
    # 6. Reporte de manipulaci√≥n de datos
    data_report = analyzer.generate_data_manipulation_report()
    
    # 7. Generar reporte PDF
    analyzer.generate_pdf_report(top_features)
    
    # 8. Guardar modelo y resultados
    analyzer.save_model_and_results()
    
    print("\nüéâ AN√ÅLISIS COMPLETO FINALIZADO")
    print("=" * 40)
    print("üìÅ Archivos generados:")
    print("   - confusion_matrix_linear.png")
    print("   - feature_importance_linear.png")
    print("   - correlation_matrix.png")
    print("   - feature_importance_report.pdf")
    print("   - linear_regression_model.pkl")
    print("   - model_results.csv")
    
    return analyzer

if __name__ == "__main__":
    analyzer = main()
